{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbemerkungen:\n",
    "Informationstheorie kann zu Beginn etwas unintuitiv wirken. Deshalb ist es hilfreich, wenn gute Visualisierungen die Vorstellungskraft unterstützen. Eine großartige - englischsprachige - Einführung (die in späteren Abschnitten allerdings über den Stoffumfang der Veranstaltung Nachrichtentechnik hinausgeht), bietet der Blog von Christopher Olah: [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/). Allerdings ist die Notation dort etwas anders als in unserer Vorlesung, weil auch für Wahrscheinlichkeitsmassefunktionen Kleinbuchstaben verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für alle Aufgaben der Informationstheorie ist es wichtig, dass die **Bayes'sche Regel für bedingte Wahrscheinlichkeiten** verstanden worden ist:\n",
    "\n",
    "\\begin{align}\n",
    "     \\mathrm{Pr}_{Y|X}(y|x) = \\frac{\\mathrm{Pr}_{X|Y}(x|y) \\cdot \\mathrm{Pr}_{Y}(y)}{\\mathrm{Pr}_{X}(x)}\n",
    "\\end{align}\n",
    "\n",
    "Diese kann durch die Definition der bedingten Wahrscheinlichkeit\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_{Y|X}(y|x) = \\frac{\\mathrm{Pr}_{Y,X}(y,x)}{\\mathrm{Pr}_{X}(x)} \\Rightarrow \\mathrm{Pr}_{Y,X}(y,x) = \\mathrm{Pr}_{Y|X}(y|x) \\cdot \\mathrm{Pr}_{X}(x)\n",
    "\\end{align}\n",
    "\n",
    "und die Symmetrie der Verbundwahrscheinlichkeit\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_{Y,X}(y,x) = \\mathrm{Pr}_{X,Y}(x,y)\n",
    "\\end{align}\n",
    "\n",
    "hergeleitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Häufig muss eine Marginalwahrscheinlichkeit berechnet werden, obwohl nur die andere Marginalwahrscheinlichkeit und die bedingte Wahrscheinlichkeit bekannt ist. Dann hilft der **Satz über die totale Wahrscheinlichkeit**:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_{Y}(y) = \\sum_{x \\in \\Omega_X} \\mathrm{Pr}_{Y,X}(y,x) = \\sum_{x \\in \\Omega_X} \\mathrm{Pr}_{Y|X}(y|x) \\cdot \\mathrm{Pr}_{X}(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn sich die durch Beobachtung der Zufallsvariablen $Y=y_i$ ergebende bedingte Wahrscheinlichkeit (Posteriorwahrscheinlichkeit) für eine Zufallsvariable $X$ von der unbedingten (A-Priori-)Wahrscheinlichkeit unterscheidet,  spiegelt das intuitiv einen Informationsgewinn wider:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_{X|Y}(x|y_i) \\neq \\mathrm{Pr}_X(x) \\quad \\Rightarrow \\quad \\text{Informationsgewinn!}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Dieser intuitive Informationsbegriff kann mithilfe der Informationstheorie mathematisch sauber definiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 18: Quellencodierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gegeben seien zwei Nachrichtenquellen $Q_1$ und $Q_2$, die Symbole aus den jeweiligen Alphabeten $X \\in \\Omega_{X} = \\{1,2,3,4\\}$ bzw. $Y \\in \\Omega_{Y} = \\{A,B,C,D\\}$ emittieren. Durch Kombination der beiden resultiert eine neue Quelle $Q$ mit dem Alphabet $\\Omega = \\{1,2,3,4\\} \\times \\{A,B,C,D\\}$, d.h. es wird jeweils ein Paar mit einer Zahl aus $\\Omega_X$ und einem Buchstaben aus $\\Omega_Y$ emittiert. Die Verbundwahrscheinlichkeiten sind fast alle bekannt und in der folgenden Tabelle dargestellt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $\\mathrm{Pr}_{X,Y}(x,y)$ |     $x=1$      |     $x=2$      |     $x=3$      |     $x=4$      |\n",
    "|:-------------------------|:--------------:|:--------------:|:--------------:|:--------------:|\n",
    "| $y=A$                    | $\\frac{1}{ 8}$ | $\\frac{1}{32}$ | $\\frac{1}{16}$ | $\\frac{1}{32}$ |\n",
    "| $y=B$                    |                | $\\frac{1}{ 8}$ | $\\frac{1}{32}$ | $\\frac{1}{32}$ |\n",
    "| $y=C$                    | $\\frac{1}{16}$ | $\\frac{1}{16}$ | $\\frac{1}{16}$ | $\\frac{1}{16}$ |\n",
    "| $y=D$                    | $\\frac{1}{ 4}$ | $     0      $ | $     0      $ | $     0      $ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1:\n",
    "Füllen Sie die noch fehlende Stelle in der Tabelle aus __und__ berechnen Sie die Randverteilungen $\\mathrm{Pr}_X(x)$ und $\\mathrm{Pr}_Y(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Verbundverteilung muss gelten, dass die Summe über alle Stellen der Funktion Eins ergibt:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}( (x,y) \\in \\Omega) = \\sum_{x \\in \\Omega_X} \\sum_{y \\in \\Omega_Y} \\mathrm{Pr}_{X,Y}(x,y) \\overset{!}{=} 1\n",
    "\\end{align}\n",
    "\n",
    "Die verbliebene Wahrscheinlichkeit ergibt sich also über:\n",
    "\n",
    "\\begin{align}\n",
    "    1 &\\overset{!}{=} 3 \\cdot 0 + \\frac{1}{4} + 2 \\cdot \\frac{1}{8} + 5 \\cdot \\frac{1}{16} + 4 \\cdot \\frac{1}{32} + \\mathrm{Pr}_{x,y}(1,B) \\\\\n",
    "    \\Rightarrow \\mathrm{Pr}_{X,Y}(1,B) &= 1 - \\left(\\frac{1}{4} + \\frac{2}{8} + \\frac{6}{16} + \\frac{4}{32} \\right) \\\\\n",
    "    &= 1 - \\frac{8 + 8 + 10 + 4}{32} = \\frac{1}{16}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Rand- oder Marginalverteilungen ergeben sich dann jeweils als Summe über alle Elemente einer Zeile ($\\mathrm{Pr}_Y(y)$) bzw. Spalte ($\\mathrm{Pr}_X(x)$) der Tabelle:\n",
    "\n",
    "| $\\mathrm{Pr}_{X,Y}(x,y)$ |     $x=1$      |     $x=2$      |     $x=3$      |     $x=4$      |$\\mathrm{Pr}_Y(y)$|\n",
    "|:-------------------------|:--------------:|:--------------:|:--------------:|:--------------:|-----------------:|\n",
    "| $y=A$                    | $\\frac{1}{ 8}$ | $\\frac{1}{32}$ | $\\frac{1}{16}$ | $\\frac{1}{32}$ | $\\frac{1}{ 4}$   |\n",
    "| $y=B$                    | $\\frac{1}{16}$ | $\\frac{1}{ 8}$ | $\\frac{1}{32}$ | $\\frac{1}{32}$ | $\\frac{1}{ 4}$   |\n",
    "| $y=C$                    | $\\frac{1}{16}$ | $\\frac{1}{16}$ | $\\frac{1}{16}$ | $\\frac{1}{16}$ | $\\frac{1}{ 4}$   |\n",
    "| $y=D$                    | $\\frac{1}{ 4}$ | $     0      $ | $     0      $ | $     0      $ | $\\frac{1}{ 4}$   |\n",
    "| $\\mathrm{Pr}_X(x)$       | $\\frac{1}{ 2}$ | $\\frac{7}{32}$ | $\\frac{5}{32}$ | $\\frac{1}{ 8}$ | $      1     $   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_xy = np.array([\n",
    "    [ 1/8, 1/32, 1/16, 1/32],\n",
    "    [1/16,  1/8, 1/32, 1/32],\n",
    "    [1/16, 1/16, 1/16, 1/16],\n",
    "    [ 1/4,    0,    0,    0],\n",
    "])\n",
    "\n",
    "assert(np.isclose(prob_xy.sum(), 1))\n",
    "\n",
    "prob_x = prob_xy.sum(axis=0)\n",
    "prob_y = prob_xy.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insbesondere ist erkennbar, dass $Y$ einer Gleichverteilung folgt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2:\n",
    "Wie groß sind die Erwartungswerte $\\mathrm{E}\\left[X\\right]$ und $\\mathrm{E}\\left[X|y=C\\right]$? Sind die Quellen $Q_1$ und $Q_2$ statistisch unabhängig?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erwartungswert von $X$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{E}\\left[X\\right] &= \\sum_{x \\in \\Omega_X} x \\cdot \\mathrm{Pr}_{X}(x) \\\\\n",
    "    &= 1 \\cdot \\frac{1}{2} + 2 \\cdot \\frac{7}{32} + 3 \\frac{5}{32} + 4 \\cdot \\frac{1}{8} \\\\\n",
    "    &= \\frac{16}{32} + \\frac{14}{32} + \\frac{15}{32} + \\frac{16}{32} \\\\\n",
    "    &= \\frac{61}{32}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erwartungswert von $X$ gegeben $y=C$:\n",
    "\n",
    "Hier muss zunächst die bedingte Wahrscheinlichkeit berechnet werden:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_{X|Y}(x|y) = \\frac{\\mathrm{Pr}_{X,Y}(x,y)}{\\mathrm{Pr}_{Y}(y)},\n",
    "\\end{align}\n",
    "\n",
    "denn $\\mathrm{E}\\left[X|y=C\\right] = \\sum_{x \\in \\Omega_X} x \\cdot \\mathrm{Pr}_{X|Y}(x|C)$\n",
    "\n",
    "Dazu kann jede der Verbundwahrscheinlichkeiten in der dritten Zeile durch $\\mathrm{Pr}_{Y}(C) = \\frac{1}{4}$ dividiert werden. Da die Elemente der Zeile zudem alle identisch sind, reicht es aus, über die Werte von X zu summieren und zu skalieren:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{E}\\left[X|y=C\\right] &= \\sum_{x \\in \\Omega_X} x \\cdot \\mathrm{Pr}_{X|Y}(x|C) \\\\\n",
    "    &= \\frac{1}{\\mathrm{Pr}_{Y}(C)} \\sum_{x \\in \\Omega_X} x \\cdot \\mathrm{Pr}_{X,Y}(x,C) \\\\\n",
    "    &= \\frac{ \\frac{1}{16}  }{\\frac{1}{4}} \\left( 1 + 2 + 3 + 4 \\right) \\\\\n",
    "    &= \\frac{10}{4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **statistische Abhängigkeit** der Quellen lässt sich auf drei Wegen zeigen:\n",
    "\n",
    "* Für statistische Unabhängigkeit muss gelten: $\\mathrm{Pr}_{X,Y}(x,y) \\overset{!}{=} \\mathrm{Pr}_{X}(x) \\cdot \\mathrm{Pr}_{Y}(y) \\quad \\forall (x,y) \\in \\Omega$.   \n",
    "  Aber z.B. $\\mathrm{Pr}_{X,Y}(4,D) = 0 \\neq \\frac{1}{4} \\cdot \\frac{1}{8} = \\mathrm{Pr}_{X}(4) \\cdot \\mathrm{Pr}_{Y}(D)$\n",
    "* Statistische Unabhängigkeit impliziert auch Unkorreliertheit.    \n",
    "  Für Unkorreliertheit muss aber z.B.  $\\mathrm{E}\\left[X|Y=C\\right] \\overset{!}{=} \\mathrm{E}\\left[X\\right]$ gelten, was ebenfalls nicht erfüllt ist.\n",
    "* Eine dritte Möglichkeit, die statistische Unabhängigkeit zu bewerten, bietet die Entropie. Dies wird in Teilaufgabe 18.3 vorgestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3:\n",
    "Wie viele Bits pro Symbol sind im Mittel hinreichend und notwendig, um die Quellen $Q_1$,\n",
    "$Q_2$ und $Q$ zu codieren? Warum sind fur die gemeinsame Codierung der Quellen weniger Bits\n",
    "notwendig, als bei der getrennten Codierung zusammen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Shannons Quellencodiertheorem gilt:\n",
    "\n",
    "> Sei $\\mathrm{H}(X)$ die Entropie der Quelle. Dann ist es für eine perfekte Rekonstruktion notwendig und hinreichend, wenn für die Codierung im Mittel $\\mathrm{H}(X)$ Bit pro Ausgangssymbol der Quelle verwendet werden.\n",
    "\n",
    "Es ist also ausreichend, einen Code zu verwenden, dessen mittlere Codewortlänge der Entropie der Quelle entspricht. Also müssen zur Beantwortung der Frage die Quellentropien berechnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiederholung: Entropie oder mittlere Unsicherheit oder mittlerer Informationsgehalt\n",
    "\n",
    "In der Informationstheorie ist der Informationsgehalt\n",
    "\n",
    "\\begin{align}\n",
    "    I(x_m) = \\log_2 \\left( \\frac{1}{\\mathrm{Pr}_{X}(x_m) } \\right) = -\\log_2 \\left(\\mathrm{Pr}_{X}(x_m) \\right)\n",
    "\\end{align}\n",
    "\n",
    "der einem Ereignis zugeordnet wird, das Maß an Überraschung, welches das Ereignis bietet. Je seltener ein Ereignis eintrifft, desto überraschender ist es. Beispielsweise ist\n",
    "\n",
    "\\begin{align}\n",
    "    I(x=4) = \\log_2 \\left( \\frac{1}{ \\mathrm{Pr}_{X}(4) } \\right) = \\log_2(8) = 3\\, \\mathrm{Bit}\n",
    "\\end{align}\n",
    "\n",
    "Dabei ist wichtig:\n",
    "* Als Funktion wird der Logarithmus verwendet, weil Produkte in Summen umgewandelt werden, sodass sich die\n",
    " **Informationsgehalte unabhängiger Quellen/Zufallsvariablen addieren** (siehe unten).\n",
    "* Damit **seltene Ereignisse einen hohen Informationsgehalt** haben (und der Informationsgehalt für diskrete Symbolalphabete positiv ist), wird der Kehrwert der Wahrscheinlichkeit genommen oder äquivalent ein negatives Vorzeichen.\n",
    "* Der Logarithmus zur Basis zwei wird verwendet, um die Anzahl binärer Entscheidungen zu benutzen.\n",
    "* Das Maß der Information ist deshalb: $1\\, \\mathrm{Bit}\\  \\hat{=} \\text{ eine binäre Entscheidung}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def information(p):\n",
    "    return -np.log2(p + (p == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wegen der Zufälligkeit der Symbole ist man am **mittleren** Informationsgehalt interessiert, also\n",
    "\n",
    "\\begin{align}\n",
    "    H(X) = \\mathrm{E}\\left[I(X)\\right] = \\sum_{x \\in \\Omega_X} I(x) \\mathrm{Pr}_X(x)\n",
    "\\end{align}\n",
    "\n",
    "Dieser Erwartungswert wird **Entropie** genannt. Der Name ist nicht zufällig gewählt, denn die aus der Thermodynamik bekannte Größe folgt dem gleichen Konzept (Dort als Maß für die Unsicherheit einer Verteilung der Teilchen. Physiker verwenden allerdings den natürlichen Logarithmus und die Boltzmannkonstante als Vorfaktor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropie der Quelle $Q_1$:\n",
    "  \n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X) &= \\sum_{x \\in \\Omega_X} -\\log_2 \\left( \\mathrm{Pr}_X(x) \\right) \\cdot \\mathrm{Pr}_X(x) \\\\\n",
    "    &= -\\left(\\frac{1}{2} \\cdot \\log_2\\left( \\frac{1}{2} \\right) + \\frac{7}{32} \\cdot \\log_2\\left( \\frac{7}{32} \\right) + \\frac{5}{32} \\cdot \\log_2\\left( \\frac{5}{32} \\right) + \\frac{1}{8} \\cdot \\log_2\\left( \\frac{1}{8} \\right) \\right) \\\\\n",
    "    &\\approx 1{,}7731\\  \\mathrm{Bit}/\\mathrm{Symbol}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X) = 1.7731 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "entropy_x = information(prob_x) @ prob_x\n",
    "print(\"H(X) = {:1.4f} Bits/Symbol\".format(entropy_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropie der Quelle $Q_2$:\n",
    "  \n",
    "Hier kommt man leichter durch Argumentation zum Ziel: $Y$ ist gleichverteilt. Alle Werte von $Y$ sind also gleich wahrscheinlich: \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_Y(y) = \\frac{1}{|\\Omega_Y|} = \\frac{1}{4}\n",
    "\\end{align}\n",
    "\n",
    "Da deshalb alle Werte des Informationsgehaltes gleich sind, ist der mittlere Informationsgehalt der Quelle $Q_2$ gleich dem Informationsgehalt eines beliebigen Zeichens $y \\in \\Omega_Y$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(Y) = -\\log_2\\left( \\frac{1}{|\\Omega_Y|} \\right) = \\log_2\\left|\\Omega_Y\\right| = \\log_2(4) = 2\\  \\mathrm{Bit}/\\mathrm{Symbol}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y) = 2.0000 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "entropy_y = information(prob_y) @ prob_y\n",
    "print(\"H(Y) = {:1.4f} Bits/Symbol\".format(entropy_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropie der Verbundquelle $Q$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X,Y) &= \\sum_{x \\in \\Omega_X} \\sum_{y \\in \\Omega_Y} -\\log_2 \\left( \\mathrm{Pr}_{X,Y}(x,y) \\right) \\cdot \\mathrm{Pr}_{X,Y}(x,y) \\\\\n",
    "    &= - \\left( 1 \\cdot \\frac{1}{4} \\cdot \\log_2\\left( \\frac{1}{4} \\right) + 2 \\cdot \\frac{1}{8} \\cdot \\log_2\\left( \\frac{1}{8} \\right) + 6 \\cdot \\frac{1}{16} \\cdot \\log_2\\left( \\frac{1}{16} \\right) + 4 \\cdot \\frac{1}{2} \\cdot \\log_2\\left( \\frac{1}{2} \\right) + 3 \\cdot 0 \\cdot \\log_2(0) \\right) \\\\\n",
    "    &= \\frac{2 \\cdot 4}{4} + \\frac{2 \\cdot 3}{8} + \\frac{6 \\cdot 4}{16} + \\frac{4 \\cdot 5}{32} + 0 \\\\\n",
    "    &= \\frac{108}{32} = \\frac{27}{8}\\ \\mathrm{Bit}/\\mathrm{Symbolpaar}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabei wurde die in der Informationstechnik übliche Konvention \"$0 \\cdot \\log_2(0)$\" $= \\lim_{x \\rightarrow 0} x \\cdot \\log_2(x) = 0$ verwendet, die durch die Regel von l'Hospital erhalten werden kann. Aus diesem Grund muss der Python-Code auch eine Sonderbehandlung der Null vornehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X,Y) = 3.3750 Bits/Symbolpaar\n"
     ]
    }
   ],
   "source": [
    "entropy_xy = (information(prob_xy) * prob_xy).sum()\n",
    "print(\"H(X,Y) = {:1.4f} Bits/Symbolpaar\".format(entropy_xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie im Abschnitt zur Definition des Informationsgehaltes erwähnt, sind Informationsgehalte (und damit auch mittlere Informationsgehalte) von statistisch unabhängigen Quellen additiv:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_{X,Y}(x,y) &= \\mathrm{Pr}_{X}(x) \\cdot \\mathrm{Pr}_{Y}(y) \\quad \\forall (x,y) \\in \\Omega \\\\\n",
    "    \\Rightarrow \\mathrm{H}(X,Y) &= \\sum_{x \\in \\Omega_x} \\sum_{y \\in \\Omega_Y} -\\log_2 \\left( \\mathrm{Pr}_{X,Y}(x,y) \\right) \\cdot \\mathrm{Pr}_{X,Y}(x,y) \\\\\n",
    "    &= \\sum_{x \\in \\Omega_X} \\sum_{y \\in \\Omega_Y} -\\log_2 \\left( \\mathrm{Pr}_{X}(x) \\cdot \\mathrm{Pr}_{Y}(y) \\right) \\cdot \\mathrm{Pr}_{X}(x) \\cdot \\mathrm{Pr}_{Y}(y) \\\\\n",
    "    &= \\sum_{x \\in \\Omega_X} \\sum_{y \\in \\Omega_Y} - \\left(\\log_2 \\left( \\mathrm{Pr}_{X}(x) \\right) + \\log_2\\left( \\mathrm{Pr}_{Y}(y) \\right) \\right) \\cdot \\mathrm{Pr}_{X}(x) \\cdot \\mathrm{Pr}_{Y}(y) \\\\\n",
    "    &= \\sum_{x \\in \\Omega_X}  -\\log_2 \\left( \\mathrm{Pr}_{X}(x) \\right) \\mathrm{Pr}_{X}(x)  \\underbrace{\\left( \\sum_{y \\in \\Omega_Y} \\mathrm{Pr}_{Y}(y) \\right)}_{=1} + \n",
    "       \\sum_{y \\in \\Omega_Y}  -\\log_2 \\left( \\mathrm{Pr}_{Y}(y) \\right) \\mathrm{Pr}_{Y}(y)  \\underbrace{\\left( \\sum_{x \\in \\Omega_Y} \\mathrm{Pr}_{X}(x) \\right)}_{=1} \\\\\n",
    "       &= \\mathrm{H}(X) + \\mathrm{H}(Y).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist für die gegebenen Quellen offensichtlich nicht erfüllt:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X) + \\mathrm{H}(Y) = 1.7731\\  \\mathrm{Bit}/\\mathrm{Symbol} + 2\\  \\mathrm{Bit}/\\mathrm{Symbol} \\neq 3.3750\\  \\mathrm{Bit}/\\mathrm{Symbol} = \\mathrm{H}(X,Y)\n",
    "\\end{align}\n",
    "\n",
    "Damit ist die statistische Abhängigkeit auch so gezeigt. Diese impliziert übrigens, dass durch Beobachtung der einen Quelle Information über die andere Quelle gewonnen wird, weil die erste Zeile immer gültig ist:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X,Y) &= \\mathrm{H}(X|Y) + \\mathrm{H}(Y) = \\mathrm{H}(Y|X) + \\mathrm{H}(X) \\\\\n",
    "    \\underset{\\text{stat. abhängig}}{\\Rightarrow} \\mathrm{H}(X) &\\neq  \\mathrm{H}(X|Y)\n",
    "\\end{align}\n",
    "\n",
    "Zum Beispiel muss $x=1$ gelten, wenn $y=D$ bekannt ist!\n",
    "\n",
    "Dies heißt aber auch, dass mehr Redundanz eliminiert werden kann, wenn die beiden Quellen $Q_1$ und $Q_2$ zusammen codiert werden, wodurch auch die mittlere Codewortlänge verringert ist. Die Anzahl verschwendeter Bits bei separater, aber sonst optimaler Codierung ist\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X) + \\mathrm{H}(Y) - \\mathrm{H}(X,Y) \\approx 0{,}3981\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}\n",
    "\n",
    "und bekommt später noch einen anderen Namen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4:\n",
    "Geben sie jeweils für $Q_1$, $Q_2$ und $Q$ einen Huffman-Code an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman-Algorithmus\n",
    "Wenn die Wahrscheinlichkeiten bekannt sind, kann der Huffman-Algorithmus durchgeführt werden, indem eine Liste der Blattknoten (also der Symbole des uncodierten Quell-Alphabets) aufsteigend nach den zugehörigen Wahrscheinlichkeiten sortiert und dann sukzessive die ersten beiden Knoten entfernt werden, während ein Vaterknoten mit der Summenwahrscheinlichkeit an die richtige Stelle sortiert wird. Die Zuordnung der Einsen und Nullen ist beliebig, sollte aber nach einer festen Konvention erfolgen.\n",
    "\n",
    "In Pseudocode sieht das so aus:\n",
    "\n",
    "\n",
    "    - Eingang: Symbole, Symbolwahrscheinlichkeiten\n",
    "    - Erstelle Knotenliste aus Symbolen, markiere alle als unerledigt\n",
    "    - Sortiere Liste nach Wahrscheinlichkeiten\n",
    "    - Solange mehr als ein Knoten nicht abgearbeitet:\n",
    "        -Erzeuge neuen unerledigten Knoten\n",
    "        -Setze Knoten als Vaterknoten für die zwei Knoten kleinster Wahrscheinlichkeit\n",
    "        -Neue Knotenwahrscheinlichkeit ist Summe der beiden Wahrscheinlichkeiten\n",
    "        -Markiere die beiden Kindknoten als erledigt\n",
    "        -Falls Kantenwahrscheinlichkeiten ungleich:\n",
    "            -Ordne dem Kindknoten höherer Wahrscheinlichkeit die \"1\" und dem anderen die \"0\" zu\n",
    "        -Sonst:\n",
    "            -Ordne dem linken Knoten die \"1\" zu und dem rechten die \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code wird als Huffman-Baum angegeben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman-Code für $Q_1$:\n",
    "![Huffman-Code für Q1](figures/A18/Huffman_X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Tabelle:\n",
    "\n",
    "| $i$ | $x_i$ | $\\mathrm{Pr}_X(x_i)$ | $c_i$   | $m_i$ | $n_{0,i}$ | $n_{1,i}$ |\n",
    "|:----|-------|----------------------|---------|-------|-----------|-----------|\n",
    "| $1$ | $1$   | $\\frac{16}{32}$      | $1_b$   | $1$   | $0$       | $1$       |\n",
    "| $2$ | $2$   | $\\frac{ 7}{32}$      | $00_b$  | $2$   | $2$       | $0$       |\n",
    "| $3$ | $3$   | $\\frac{ 5}{32}$      | $011_b$ | $3$   | $1$       | $2$       |\n",
    "| $4$ | $4$   | $\\frac{ 4}{32}$      | $010_b$ | $3$   | $2$       | $1$       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman-Code für $Q_2$:\n",
    "![Huffman-Code für Q2](figures/A18/Huffman_Y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Tabelle:\n",
    "\n",
    "| $i$ | $y_i$ | $\\mathrm{Pr}_Y(y_i)$ | $c_i$   | $m_i$ | $n_{0,i}$ | $n_{1,i}$ |\n",
    "|:----|-------|----------------------|---------|-------|-----------|-----------|\n",
    "| $1$ | A     | $\\frac{1}{4}$        | $11_b$  | $2$   | $0$       | $2$       |\n",
    "| $2$ | B     | $\\frac{1}{4}$        | $10_b$  | $2$   | $1$       | $1$       |\n",
    "| $3$ | C     | $\\frac{1}{4}$        | $01_b$  | $2$   | $1$       | $1$       |\n",
    "| $4$ | D     | $\\frac{1}{4}$        | $00_b$  | $2$   | $2$       | $0$       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman-Code für $Q$:\n",
    "![Huffman-Code für Q2](figures/A18/Huffman_XY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Tabelle:\n",
    "\n",
    "| $i,j$ | $(x_i,y_j)$ | $\\mathrm{Pr}_{X,Y}(x_i, y_j)$ | $c_{ij}$   | $m_{ij}$ | $n_{0,ij}$ | $n_{1,ij}$ |\n",
    "|:------|-------------|-------------------------------|------------|----------|------------|------------|\n",
    "| $ 1$  | $2A$        | $\\frac{1}{32}$                | $11111_b$  | $5$      | $0$        | $5$        |\n",
    "| $ 2$  | $4A$        | $\\frac{1}{32}$                | $11110_b$  | $5$      | $1$        | $4$        |\n",
    "| $ 3$  | $3B$        | $\\frac{1}{32}$                | $11101_b$  | $5$      | $1$        | $4$        |\n",
    "| $ 4$  | $4B$        | $\\frac{1}{32}$                | $11100_b$  | $5$      | $2$        | $3$        |\n",
    "| $ 5$  | $3A$        | $\\frac{1}{16}$                | $1101_b$   | $4$      | $1$        | $3$        |\n",
    "| $ 6$  | $1B$        | $\\frac{1}{16}$                | $1100_b$   | $4$      | $2$        | $2$        |\n",
    "| $ 7$  | $1C$        | $\\frac{1}{16}$                | $1011_b$   | $4$      | $1$        | $3$        |\n",
    "| $ 8$  | $2C$        | $\\frac{1}{16}$                | $1010_b$   | $4$      | $2$        | $2$        |\n",
    "| $ 9$  | $3C$        | $\\frac{1}{16}$                | $1001_b$   | $4$      | $2$        | $2$        |\n",
    "| $10$  | $4C$        | $\\frac{1}{16}$                | $1000_b$   | $4$      | $3$        | $1$        |\n",
    "| $11$  | $1A$        | $\\frac{1}{ 8}$                | $011_b$    | $3$      | $1$        | $2$        |\n",
    "| $12$  | $2B$        | $\\frac{1}{ 8}$                | $010_b$    | $3$      | $2$        | $1$        |\n",
    "| $13$  | $1D$        | $\\frac{1}{ 4}$                | $00_b$     | $2$      | $2$        | $0$        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.5:\n",
    "Wie groß ist die mittlere Codewortlänge jedes Huffman-Codes? Wie groß ist die Redundanz?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Einschub: Bedeutung der mittleren Codewortlänge und der Redundanz\n",
    "Aus der Vorlesung ist bekannt, dass der Huffman-Code nur dann redundanzfrei ist, wenn die Wahrscheinlichkeiten der Symbole sämtlich aus Zweierpotenzen zusammengesetzt sind. Dies liegt ander Struktur als binärer Baum: Weil immer genau zwei Knoten zusammengefasst werden, findet an jedem Nichtblattknoten eine binäre Entscheidung statt.\n",
    "\n",
    "Dies lässt sich auch so interpretieren: Die Huffman-Algorithmus **unterstellt** Zweierpotenz-Wahrscheinlichkeiten und wählt die Länge des Codewortes dann gleich dem zugehörigen **unterstellten** Informationsgehalt. Sei $Q$ die angenommene Verteilung der Symbole. Dann gilt:\n",
    "\n",
    "\\begin{align} \n",
    "    Q_X(x) &= \\frac{1}{2^{m_i}} \\\\\n",
    "    \\Rightarrow  I_Q(x) &= -\\log_2(Q_X(x)) = \\log_2 \\left(2^{m_i} \\right) = M_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die mittlere Codewortlänge ist nun der **Erwartungswert des unterstellten Informationsgehaltes unter der tatsächlichen Verteilung** $\\mathrm{Pr}_X(x)$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{E}\\left[ I_Q(X)\\right] &= \\sum_{x_i \\in \\Omega_X} \\log_2 \\left(2^{m_i} \\right) \\mathrm{Pr}_X(x_i) \\\\\n",
    "    &= \\sum_{x_i \\in \\Omega_X} m_i \\mathrm{Pr}_X(x_i) \\\\\n",
    "    &= \\mathrm{E}\\left[ M \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es lässt sich beweisen, dass die mittlere Codewortlänge immer **größer oder gleich** der Entropie ist. Deshalb ist die Redundanz, also die **mittlere Menge an bei der Codierung verschwendeten Bits pro Symbol**, auch immer positiv:\n",
    "\n",
    "\\begin{align}\n",
    "    R = \\mathrm{E}\\left[ M \\right] - \\mathrm{H}(X) = \\mathrm{E}\\left[ \\log_2 \\left(\\frac{\\mathrm{Pr}_X(X)}{Q_X(X)} \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "Der Grund dafür ist, dass die Werte, an denen der Logarithmus im Argument negativ ist, eine geringere Wahrscheinlichkeit $\\mathrm{Pr}_X(x)$ haben als die Werte, bei denen er negativ ist. Die Redundanz kann übrigens als (asymmetrisches) Ähnlichkeitsmaß zwischen den beiden Verteilungen gesehen werden: Sind wahre und unterstellte Verteilung gleich, ist die Redundanz Null. Je unähnlicher sie sich sind, desto größer wird die Redundanz. Die Transinformation steht in einem engen Zusammenhang mit der Redundanz.\n",
    "\n",
    "Wenn beide Größen berechnet wurden, sollte die Redundanz aber einfach als Differenz der Werte berechnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### $Q_1$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{E}\\left[M_1\\right] &= \\sum_{x_i \\in \\Omega_X} m_i \\mathrm{Pr}_X(x_i) \\\\\n",
    "        &= \\left(1 \\cdot \\frac{16}{32} + 2 \\cdot \\frac{7}{32} + 3 \\cdot \\frac{5}{32} + 3 \\cdot \\frac{4}{32}\\right)\\,\\mathrm{Bit}/\\mathrm{Symbol} \\\\\n",
    "        &= \\frac{16 + 14 + 27}{32}\\,\\mathrm{Bit}/\\mathrm{Symbol} = \\frac{57}{32}\\,\\mathrm{Bit}/\\mathrm{Symbol} \\approx 1{,}7813\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    R_1 &= \\mathrm{E}\\left[M_1\\right] - \\mathrm{H}(X) \\\\\n",
    "        &= 1{,}7813\\,\\mathrm{Bit}/\\mathrm{Symbol} - 1{,}7731\\  \\mathrm{Bit}/\\mathrm{Symbol} \\\\\n",
    "        &= 0{,}0082\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[M_1] = 1.7812 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "codeword_length_x = np.array([1, 2, 3, 3])\n",
    "avg_code_length_x = codeword_length_x @ prob_x\n",
    "print(\"E[M_1] = {:1.4f} Bits/Symbol\".format(avg_code_length_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_1 = 0.0082 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "redundancy_x = avg_code_length_x - entropy_x\n",
    "print(\"R_1 = {:1.4f} Bits/Symbol\".format(redundancy_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Q_2$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{E}\\left[M_2\\right] &= \\sum_{y_i \\in \\Omega_Y} m_i \\mathrm{Pr}_Y(y_i) \\\\\n",
    "        &= \\left(2 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{1}{4} \\right)\\,\\mathrm{Bit}/\\mathrm{Symbol} \\\\\n",
    "        &= \\frac{8}{4}\\,\\mathrm{Bit}/\\mathrm{Symbol} = 2\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    R_2 &= \\mathrm{E}\\left[M_2\\right] - \\mathrm{H}(Y) \\\\\n",
    "        &= 2\\mathrm{Bit}/\\mathrm{Symbol} - 2\\mathrm{Bit}/\\mathrm{Symbol} \\\\\n",
    "        &= 0\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[M_2] = 2.0000 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "codeword_length_y = np.array([2, 2, 2, 2])\n",
    "avg_code_length_y = codeword_length_y @ prob_y\n",
    "print(\"E[M_2] = {:1.4f} Bits/Symbol\".format(avg_code_length_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_2 = 0.0000 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "redundancy_y = avg_code_length_y - entropy_y\n",
    "print(\"R_2 = {:1.4f} Bits/Symbol\".format(redundancy_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $Q_3$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{E}\\left[M\\right] &= \\sum_{x_i \\in \\Omega_X} \\sum_{y_j \\in \\Omega_Y} m_{ij} \\mathrm{Pr}_{X,Y}(x_i,y_j) \\\\\n",
    "        &= \\left(4 \\cdot 5 \\cdot \\frac{1}{32} + 6 \\cdot 4 \\cdot \\frac{1}{16} + 2 \\cdot 3 \\frac{1}{8} + 1 \\cdot 2 \\cdot \\frac{1}{4} \\right) \\\\\n",
    "        &= \\frac{20 + 48 + 24 + 16 }{32} = \\frac{108}{32}\\,\\mathrm{Bit}/\\text{Symbolpaar}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    R &= \\mathrm{E}\\left[M\\right] - \\mathrm{H}(X,Y) \\\\\n",
    "        &= 0\\,\\mathrm{Bit}/\\text{Symbolpaar}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Berechnung der Redundanz ergibt, dass der Huffman-Code im Falle von Wahrscheinlichkeiten in Form negativer Zweierpotenzen optimal, d.h. redundanzfrei ist. Dies ist gültig für $Q_2$ und $Q$, obwohl es für $Q_1$ nicht gilt. Durch gemeinsame Codierung von mehreren Quellen (oder z.B. Symbolpaaren, -tripeln, ... einer Quelle) kann die Redundanz des Codes also unter Umständen verringert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[M] = 3.3750 Bits/Symbolpaar\n"
     ]
    }
   ],
   "source": [
    "codeword_length_xy = np.array([\n",
    "    [3, 5, 4, 5],\n",
    "    [4, 3, 5, 5],\n",
    "    [4, 4, 4, 4],\n",
    "    [2, 0, 0, 0]\n",
    "])\n",
    "avg_code_length_xy = (codeword_length_xy * prob_xy).sum()\n",
    "print(\"E[M] = {:1.4f} Bits/Symbolpaar\".format(avg_code_length_xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.0000 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "redundancy_xy = avg_code_length_xy - entropy_xy\n",
    "print(\"R = {:1.4f} Bits/Symbol\".format(redundancy_xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.6:\n",
    "Wie groß ist die Wahrscheinlichkeit für eine binäre $1$ in den Codes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Wahrscheinlichkeit für eine binäre $1$ zu erhalten, wird die mittlere Anzahl an Einsen pro Codewort berechnet und durch die mittlere Anzahl an Bits pro Codewort geteilt:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}(1) = \\frac{1}{\\mathrm{E}\\left[M\\right]}\\sum_{x_i \\in \\Omega_X} n_{1,i} \\mathrm{Pr}_X(x_i)\n",
    "\\end{align}\n",
    "\n",
    "Allerdings ist die Anzahl der Einsen oder Nullen pro Codewort von der Konvention der Bitvergabe an die Zweige des Huffman-Baumes abhängig. Mit der Konvention im Pseudocode von Teilaufgabe 4 ergibt sich:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Q_1$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{Pr}_1(1) &= \\frac{1}{\\frac{57}{32}} \\left( 1 \\cdot \\frac{16}{32} + 2 \\cdot \\frac{5}{32} + 1 \\cdot \\frac{4}{32} \\right) \\\\\n",
    "    &= \\frac{1}{\\frac{57}{32}} \\frac{30}{32} = \\frac{30}{57} \\approx 0{,}5263\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr_1(1) = 0.5263\n"
     ]
    }
   ],
   "source": [
    "n_1_x = np.array([1 , 0, 2, 1])\n",
    "avg_1_x = n_1_x @ prob_x\n",
    "prob_1_x = avg_1_x/avg_code_length_x\n",
    "print(\"Pr_1(1) = {:1.4f}\".format(prob_1_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Q_2$ und $Q$:\n",
    "\n",
    "Die beiden anderen Quellen sind redundanzfrei codiert. In diesem Fall ist die Wahrscheinlichkeit für eine binäre Eins gleich der Wahrscheinlichkeit für eine binäre Null gleich $1/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr_2(1) = 0.5000\n"
     ]
    }
   ],
   "source": [
    "n_1_y = np.array([2 , 1, 1, 0])\n",
    "avg_1_y = n_1_y @ prob_y\n",
    "prob_1_y = avg_1_y/avg_code_length_y\n",
    "print(\"Pr_2(1) = {:1.4f}\".format(prob_1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(1) = 0.5000\n"
     ]
    }
   ],
   "source": [
    "n_1_xy = codeword_length_xy = np.array([\n",
    "    [2, 5, 3, 4],\n",
    "    [2, 1, 4, 3],\n",
    "    [3, 2, 2, 1],\n",
    "    [0, 0, 0, 0]\n",
    "])\n",
    "avg_1_xy = (n_1_xy * prob_xy).sum()\n",
    "prob_1_xy = avg_1_xy/avg_code_length_xy\n",
    "print(\"Pr(1) = {:1.4f}\".format(prob_1_xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4:\n",
    "Wie viele Bits pro Symbol sind im Mittel hinreichend und notwendig, um die Quelle $Q_1$ zu codieren, **wenn** $Q_2$ **bekannt** ist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn $Y=y_j$ als bekannt angenommen ist, ist es nicht mehr zufällig. Deshalb muss mit den bedingten Wahrscheinlichkeiten gerechnet werden:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X|Y=y_j) = \\sum_{x_i \\in \\Omega_X} -\\log_2\\left(\\mathrm{Pr}_{X|Y}(x_i|y_j)\\right)\\mathrm{Pr}_{X|Y}(x_i|y_j)\n",
    "\\end{align}\n",
    "\n",
    "Da allerdings $Y$ tatsächlich immer noch zufällig ist, muss über alle möglichen Fälle, in denen $Y$ bekannt sein kann, gemittelt werden:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X|Y) &= \\mathrm{E}_{Y}\\left[\\mathrm{H}(X|Y=y_j)\\right] =\\sum_{y_j \\in \\Omega_Y} \\left(\\sum_{x_i \\in \\Omega_X} -\\log_2\\left(\\mathrm{Pr}_{X|Y}(x_i|y_j)\\right)\\mathrm{Pr}_{X|Y}(x_i|y_j)\\right) \\mathrm{Pr}_{Y}(y_j) \\\\\n",
    "    &=\\mathrm{E}_{X,Y}\\left[ -\\log_2\\left(\\mathrm{Pr}_{X|Y}(x_i|y_j)\\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "Dies ist die bedingt Entropie von $X$ gegeben $Y$, also das Maß an Restunsicherheit in $Q_1$, wenn $Q_2$ bekannt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit ein paar Umformungen unter Zuhilfenahme der Bayesschen Regel lässt sich die Berechnung der bedingten Wahrscheinlichkeiten allerdings vermeiden:\n",
    "\n",
    "\\begin{align}\n",
    "     \\mathrm{H}(X|Y) &= \\sum_{y_j \\in \\Omega_Y} \\left(\\sum_{x_i \\in \\Omega_X} -\\log_2\\left(\\frac{\\mathrm{Pr}_{X,Y}(x_i,y_j)}{\\mathrm{Pr}_{Y}(y_j)}\\right)\\mathrm{Pr}_{X|Y}(x_i|y_j)\\right) \\mathrm{Pr}_{Y}(y_j) \\\\\n",
    "    &= \\sum_{y_j \\in \\Omega_Y} \\sum_{x_i \\in \\Omega_X} \\left(-\\log_2\\left(\\mathrm{Pr}_{X,Y}(x_i,y_j)\\right) + \\log_2\\left( \\mathrm{Pr}_{Y}(y_j) \\right)\\right) \\mathrm{Pr}_{X|Y}(x_i|y_j)\\mathrm{Pr}_{Y}(y_j) \\\\\n",
    "    &= \\sum_{y_j \\in \\Omega_Y} \\sum_{x_i \\in \\Omega_X} -\\log_2\\left(\\mathrm{Pr}_{X,Y}(x_i,y_j)\\right) \\mathrm{Pr}_{X,Y}(x_i,y_j)\\quad - \\quad\\sum_{y_j \\in \\Omega_Y} -\\log_2\\left( \\mathrm{Pr}_{Y}(y_j) \\right) \\underbrace{\\left(\\sum_{x_i \\in \\Omega_X} \\mathrm{Pr}_{X|Y}(x_i|y_j) \\right)}_{=1} \\mathrm{Pr}_{Y}(y_j) \\\\\n",
    "    &= \\mathrm{H}(X,Y) - \\mathrm{H}(Y) \\\\\n",
    "    &= \\frac{27}{8}\\,\\mathrm{Bit}/\\mathrm{Symbolpaar} - 2\\,\\mathrm{Bit}/\\mathrm{Symbol} \\\\\n",
    "    &= \\frac{11}{8}\\,\\mathrm{Bit}/\\mathrm{Symbol} = 1{,}375\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insbesondere ergibt sich damit:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X,Y) = \\mathrm{H}(X|Y) - \\mathrm{H}(Y) = \\mathrm{H}(Y|X) - \\mathrm{H}(X)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. H(X|Y) = E[-log2(Pr(X|Y))] = 1.3750 Bits/Symbol\n",
      "2. H(X|Y) = H(X,Y) - H(Y)     = 1.3750 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "cond_prob_x_given_y = prob_xy/prob_y\n",
    "\n",
    "cond_entropy_x_given_y = (information(cond_prob_x_given_y) * prob_xy).sum()\n",
    "print(\"1. H(X|Y) = E[-log2(Pr(X|Y))] = {:1.4f} Bits/Symbol\".format(cond_entropy_x_given_y))\n",
    "print(\"2. H(X|Y) = H(X,Y) - H(Y)     = {:1.4f} Bits/Symbol\".format(entropy_xy - entropy_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.8:\n",
    "Wieviel Information in Bits pro Symbol erfährt man über $Q_1$ **durch die Beobachtung von $Q_2$**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Frage ist sehr eng mit der vorherigen Teilaufgabe verknüpft:   \n",
    "Wenn $Q_2$ und damit der Wert von $Y$ bekannt ist, muss nur noch der Anteil der Information in  $Q_1$ codiert werden, der der Restunsicherheit entspricht, welche nach der letzten Aufgabenstellung ja gerade die bedingte Entropie $\\mathrm{H}(X|Y)$ ist.   \n",
    "Die gewonnene Information ist nun die Differenz zwischen der Entropie und der bedingten Entropie von $X$, denn genau dieser Anteil der Entropie, also der Unsicherheit in $X$, ist durch die Beobachtung sicher geworden. Also gilt:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{I}(X;Y) = \\mathrm{H}(X) \\ - \\ \\mathrm{H}(X|Y)\n",
    "\\end{align}\n",
    "\n",
    "Dies ist die sogenannte **Wechselseitige Information** oder **Transinformation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im gegebenen Fall beträgt sie:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{I}(X;Y) = \\mathrm{H}(X) \\ - \\ \\mathrm{H}(X|Y) = \\left( 1{,}7731 - \\frac{11}{8} \\right) \\,\\mathrm{Bit}/\\mathrm{Symbol} \\approx 0{,}3981\\,\\mathrm{Bit}/\\mathrm{Symbol}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es müssen im Mittel also ungefähr $0{,}4\\,\\mathrm{Bit}/\\mathrm{Symbol}$ weniger codiert werden, wenn $Q_2$ bekannt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Die Transinformation kann im Übrigen auch (durch Einsetzen) mithilfe der Verbundentropie berechnet werden:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{I}(X;Y) &= \\mathrm{H}(X) \\ - \\left( \\mathrm{H}(X,Y) - \\mathrm{H}(Y)\\right) \\\\\n",
    "                    &= \\mathrm{H}(X) + \\mathrm{H}(Y) - \\mathrm{H}(X,Y)\n",
    "\\end{align}\n",
    "\n",
    "und ist damit symmetrisch:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{H}(X) \\ - \\ \\mathrm{H}(X|Y) = \\mathrm{I}(X;Y) = \\mathrm{I}(Y;X) = \\mathrm{H}(Y) \\ - \\ \\mathrm{H}(Y|X).\n",
    "\\end{align}\n",
    "\n",
    "Sie kann auch als folgender Erwartungswert berechnet werden:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{I}(X;Y) = \\mathrm{E}_{X,Y}\\left[ \\log_2\\left( \\frac{\\mathrm{Pr}_{X,Y}(X,Y)}{ \\mathrm{Pr}_{X}(X) \\cdot \\mathrm{Pr}_{Y}(Y)}\\right) \\right],\n",
    "\\end{align}\n",
    "\n",
    "und ist dann die Redundanz, die sich ergibt, wenn man $X$ und $Y$ getrennt unter der Annahme statistischer Unabhängigkeit codiert. Sie misst also die Unähnlichkeit der Verbundverteilung zum Produkt der Einzelverteilungen, d.h. sie ist ein Maß für die statistische Abhängigkeit der Variablen. Durch Python wird die letzte der Formen ausgerechnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I(X;Y) = 0.3981 Bits/Symbol\n"
     ]
    }
   ],
   "source": [
    "mutual_information_xy = (-information(prob_xy/(prob_x*prob_y))*prob_xy).sum()\n",
    "print(\"I(X;Y) = {:1.4f} Bits/Symbol\".format(mutual_information_xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.9:\n",
    "Warum sind die folgenden Codes keine Huffman-Codes?\n",
    "\n",
    "* $\\{0, 11, 101, 110, 111, 1000, 1001\\}$\n",
    "* $\\{00, 01, 11, 101, 1000\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Huffman-Code ist konstruktionsbedingt immer präfixfrei, d.h. kein Codewort ist der Beginn eines anderen Codewortes. Dies wird vom ersten Code nicht erfüllt, denn $11$ ist z.B. der Beginn von $110$ und $111$. Bei der Dekodierung des Bitstroms könnte also gar nicht ohne weiteres festgestellt werden, ob die Sequenz $(11,0)$ oder das Codewort $110$ gesendet wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der zweite Code erfüllt zwar die Präfixfreiheit, hat aber unnötig lange Codewörter. Ein Blick auf den Code-Baum verschafft hier Klarheit:\n",
    "\n",
    "![Codebaum, der kein Huffman-Baum ist](figures/A18/Not_Huffman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch den Huffman-Algorithmus kann nur ein Baum entstehen, bei dem ein Knoten    \n",
    "**entweder**\n",
    "* Ein Blattknoten ist, also direkt ein Symbol repräsentiert   \n",
    "\n",
    "**oder**\n",
    "\n",
    "* Zwei Kindknoten (bzw. zwei absteigende Zweige) besitzt.\n",
    "\n",
    "Dies ist beim gegebenen Baum offensichtlich nicht erfüllt, also kann er kein Huffman-Baum und der zugehörige Code damit kein Huffman-Code sein!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
